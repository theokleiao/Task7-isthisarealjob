# -*- coding: utf-8 -*-
"""NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1m5byuGib6MpHbMwINeFB-8vgxyf18azW
"""

import re
import nltk
import pandas as pd

from google.colab import files
uploaded = files.upload()

itarj_df = pd.read_csv('posts.csv')
itarj_df.head()

itarj_body = itarj_df.body
itarj_body.head()

stop = set(stopwords.words('english')) 
print(stop)

#data wrangling
#snow = nltk.stem.Stemmer('english')
itarj_b = []
for sentence in itarj_body:
    sentence = sentence.lower()                 # Converting to lowercase
    cleanr = re.compile(r'<.*?>')
    sentence = re.sub(cleanr, ' ', sentence)        #Removing HTML tags
    sentence = re.sub(r'[?|!|@|\'|"|#]',r' ',sentence)
    sentence = re.sub(r'[.|,|)|(|\|/]',r' ',sentence)        #Removing Punctuations
    sentence = re.sub(r'(\d{10})',r' ',sentence)              # Removing phone number
    sentence = re.sub(r'([A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,4})',r' ',sentence)        #Removing email addresses

for word in sentence.split():
   if word not in stopwords.words('english'):   # Stemming and removing stopwords
    itarj_b.append(itarj_body)
print(itarj_b)

#tokenization and filetering
from nltk.tokenize import word_tokenize
nltk.download('punkt')
tokenized_itarj = word_tokenize(str(itarj_b))
#print(tokenized_itarj)
for w in tokenized_itarj:
    if w not in stop_words:
        tokenized_itarj.append(w)
print("Tokenized Sentence:",tokenized_itarj)

from nltk.stem import PorterStemmer
ps = PorterStemmer()
itarj_b = ps.stem(tokenized_itarj)    
#final_itarj = itarj_b

freq = nltk.FreqDist(tokenized_itarj) 
for key,val in freq.items(): 
    print (str(key) + ':' + str(val))